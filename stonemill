#!/usr/bin/env python3

import os
import sys
import re
import argparse


class Definition(object):
	def __init__(self, filepath, text, append=False):
		super(Definition, self).__init__()
		self.filepath = filepath
		self.text = text
		self.append = append
	def but_replace(self, key, replacement):
		return Definition(filepath=self.filepath, append=self.append, text=self.text.replace(key, replacement))
	def and_append(self, addendum):
		return Definition(filepath=self.filepath, append=self.append, text=self.text + addendum)
	def substitute_filepath(self, **args):
		t = self.filepath
		for k in args:
			t = t.replace('{{'+k+'}}', args[k])
		return t
	def substitute_text(self, **args):
		t = self.text
		for k in args:
			t = t.replace('{{'+k+'}}', args[k])
		return t
	def check_file(self, **args):
		filepath = self.substitute_filepath(**args)
		if self.append:
			if not os.path.exists(filepath):
				raise Exception('append file missing: ' + filepath)
		else:
			if os.path.exists(filepath):
				raise Exception('write file already exists: ' + filepath)
	def write_file(self, **args):
		filepath = self.substitute_filepath(**args)
		text = self.substitute_text(**args)
		if self.append:
			print('[+] appending to', filepath)
			with open(filepath, 'a') as f:
				f.write(text)
		else:
			if not os.path.exists(os.path.dirname(filepath)):
				print('[+] making directory', os.path.dirname(filepath))
				os.makedirs(os.path.dirname(filepath))
			print('[+] writing to', filepath)
			with open(filepath, 'w') as f:
				f.write(text)

class TemplateDefinitions(object):
	def __init__(self, message, arguments, definitions, post_message):
		super(TemplateDefinitions, self).__init__()
		self.message = message
		self.arguments = arguments
		self.definitions = definitions
		self.post_message = post_message

	def parse_arguments(self, **template_args):
		parsed_args = {}
		for k in self.arguments:
			if template_args.get(k) is None:
				raise Exception('error: missing argument: ' + k)
			if not re.match(self.arguments[k], template_args[k]):
				raise Exception('error: invalid ' + k + ' arugment: ' + template_args[k])
			parsed_args[k] = template_args[k]
		return parsed_args
				
	def mill_template(self, template_args):
		name = self.message.format(**template_args)
		print('[[preping {}...]]'.format(name))
		for d in self.definitions:
			d.check_file(**template_args)
		print('[[milling {}...]]'.format(name))
		for d in self.definitions:
			d.write_file(**template_args)
		print('[[done {}!]]'.format(name))
		print(self.post_message.format(**template_args))


base_terraform_main = Definition("infrastructure/main.tf", text='''provider "aws" {
  region = "us-east-1"

  default_tags {
    tags = {
      Name = "${local.infragroup_fullname}"
    }
  }
}

variable "company_name" { default = "{{company_name}}" }
variable "group_name" { default = "{{infra_name}}" }
variable "stage" { default = "beta" }

locals {
  metrics_path = "${var.company_name}/${local.infragroup_fullname}"
  infragroup_fullname = "${var.group_name}-${var.stage}"
}

# resource group to track active infrastructure resources
resource "aws_resourcegroups_group" "infra_resource_group" {
name = "${local.infragroup_fullname}-resourcegroup"

  resource_query {
    query = <<JSON
{
  "ResourceTypeFilters": ["AWS::AllSupported"],
  "TagFilters": [
    {
      "Key": "Name",
      "Values": ["${local.infragroup_fullname}"]
    }
  ]
}
JSON
  }
}

# dashboard for monitoring the overall system. add widgets as necessary
resource "aws_cloudwatch_dashboard" "service_monitoring_dashboard" {
  dashboard_name = "${local.infragroup_fullname}-dashboard"
  dashboard_body = <<EOF
{
  "widgets": []
}
EOF
}
''')

base_makefile = Definition("./Makefile", text='''
include .env
export

configure:
	@aws configure set aws_access_key_id ${AWS_ACCESS_ID}
	@aws configure set aws_secret_access_key ${AWS_ACCESS_KEY}
	./scripts/aws-authenticate-mfa.sh

bash: configure
	bash

deploy:
	./scripts/deploy.sh -deploy

destroy:
	./scripts/deploy.sh -destroy

clean:
	rm -rf build .keys infrastructure/.terraform */modules */node_modules */dist infrastructure/terraform.tfstate*

build:

''')

base_lambda_module = Definition("infrastructure/main.tf", append=True, text='''
variable "{{lambda_name}}_build_path" { default = "../build/{{lambda_name}}.zip" }

module "{{lambda_name}}" {
  source = "./{{lambda_name}}"

  metrics_path = local.metrics_path
  infragroup_fullname = local.infragroup_fullname
  lambda_build_path = var.{{lambda_name}}_build_path
}
''')

base_lambda_makefile = Definition("./Makefile", append=True, text='''
build_{{lambda_name}}:
	-mkdir build
	cd {{lambda_name}} && bash ./build.sh
''')

base_lambda_definition = Definition("infrastructure/{{lambda_name}}/{{lambda_name}}.tf", text='''
variable "metrics_path" { type = string }
variable "infragroup_fullname" { type = string }
variable "lambda_build_path" { type = string }

locals {
  fullname = "${var.infragroup_fullname}-{{lambda_name}}"
  metrics_group = "${var.metrics_path}/{{lambda_name}}"
}

resource "aws_lambda_function" "lambda_function" {
  function_name = "${local.fullname}"
  publish = true

  runtime = "python3.8"
  handler = "main.lambda_handler"

  filename = var.lambda_build_path
  source_code_hash = filebase64sha256(var.lambda_build_path)

  role = aws_iam_role.lambda_execution_role.arn

  memory_size = 512
  timeout = 30

  environment {
    variables = {
      INFRAGROUP_FULLNAME = var.infragroup_fullname
      METRICS_GROUP = local.metrics_group
    }
  }
}

resource "aws_lambda_function_event_invoke_config" "lambda_function_invoke_config" {
  function_name = aws_lambda_function.lambda_function.function_name
  maximum_retry_attempts = 0
}

resource "aws_cloudwatch_log_group" "lambda_function_cloudwatch" {
  name = "/aws/lambda/${aws_lambda_function.lambda_function.function_name}"
  retention_in_days = 90
}

resource "aws_iam_role" "lambda_execution_role" {
  name = "${local.fullname}-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Sid    = ""
      Principal = {
        Service = "lambda.amazonaws.com"
      }
    }]
  })
}

resource "aws_iam_policy" "lambda_policy" {
  name        = "${local.fullname}-policy"
  path        = "/"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Action = "logs:CreateLogGroup"
      Resource = "*"
    }, {
      Effect = "Allow"
      Action = [ "logs:CreateLogStream", "logs:PutLogEvents" ]
      Resource = "*"
    }, {
      Effect = "Allow"
      Action = "cloudwatch:PutMetricData"
      Resource = "*"
      Condition = {
        StringEquals = {
          "cloudwatch:namespace" = local.metrics_group
        }
      }
    }]
  })
}

resource "aws_iam_role_policy_attachment" "lambda_policy_attachment" {
  role       = aws_iam_role.lambda_execution_role.name
  policy_arn = aws_iam_policy.lambda_policy.arn
}

# # crash alarm, will alarm when the lambda crashes recently
# resource "aws_cloudwatch_metric_alarm" "crash_alarm" {
#   alarm_name = "${local.fullname}-crash_alarm"
#   comparison_operator = "GreaterThanOrEqualToThreshold"
#   evaluation_periods = "1"
#   metric_name = "lambda-crash"
#   namespace = local.metrics_group
#   period = "300"
#   statistic = "Sum"
#   threshold = "1"
#   alarm_actions = [var.sns_alarm_topic_arn]
# }

output "monitor_widget_json" {
  value = <<EOF
  {
    "height": 6,
    "width": 6,
    "type": "metric",
    "properties": {
      "metrics": [
        [ "${local.metrics_group}", "lambda-success", { "color": "#98df8a", "label": "lambda-success" } ],
        [ ".", "lambda-error", { "color": "#ffbb78" } ],
        [ ".", "lambda-crash", { "color": "#ff9896" } ]
      ],
      "view": "timeSeries",
      "stacked": true,
      "region": "us-east-1",
      "stat": "Sum",
      "period": 300,
      "title": "{{lambda_name}} metrics"
    }
  }
EOF
}

''')

base_lambda_main = Definition("{{lambda_name}}/main.py", text='''import sys
sys.path.append('modules')
import os
import json
import boto3
import traceback

import lambda_routing
import routes



# aws api clients
cloudwatch_client = boto3.client('cloudwatch')
# env variables passed in from infrastructure
metrics_group = os.environ.get('METRICS_GROUP')



def lambda_handler(event, context):
	print('[i] handling event:', event)
	try:
		# perform routing
		res = route_request(event)
		print('[r] response:', res)
		if res.get('success') is not None and res['success']:
			send_metric('lambda-success')
		else:
			send_metric('lambda-error')
		return res
	except Exception as e:
		print('[!] exception thrown:', e, traceback.format_exception(None, e, e.__traceback__))
		# track crash metrics
		send_metric('lambda-crash')
		res = { 'success': False, 'error': 'server-side error' }
		print('[r] response:', res)
		return res

def send_metric(name):
	cloudwatch_client.put_metric_data(Namespace=metrics_group,
		MetricData = [{
			'MetricName': name,
			'Unit': 'Count',
			'Value': 1
		}])

def route_sqs_request(event):
	results = []
	for record in event['Records']:
		results.append(route_request(record))
	return results

def route_request(event):
	# get the json data from the body
	try:
		data = json.loads(str(event['body']))
	except ValueError as e:
		print('[!] error parsing json body:', e)
		return { 'success': False, 'error': 'invalid request' }

	# parse the action
	if data.get('action') is None:
		print('[!] missing action')
		return { 'success': False, 'error': 'invalid request' }
	action = str(data['action'])

	# try to route it
	route_fun = lambda_routing.get_route(action)
	if route_fun is None:
		print('[!] invalid action: ', action)
		return { 'success': False, 'error': 'invalid request' }
	
	# handle the routing
	print('[i] executing route: ' + action)
	return route_fun(data)
''')

base_lambda_lib = Definition("{{lambda_name}}/lambda_routing.py", text='''all_lambda_routes = {}
def lambda_route(route):
	def lambda_route_lambda_route(f):
		def wrapper(*args, **kwargs):
			return f(*args, **kwargs)
		all_lambda_routes[route] = wrapper
		return wrapper
	return lambda_route_lambda_route

def authenticated_lambda_route(route, authentication_fun):
	def lambda_route_lambda_route(f):
		def wrapper(data, *args, **kwargs):
			account = authentication_fun(data)
			if account is None:
				return { 'success': False, 'error': 'unauthenticated' }
			else:
				return f(data, account, *args, **kwargs)
		all_lambda_routes[route] = wrapper
		return wrapper
	return lambda_route_lambda_route

def get_route(route):
	return all_lambda_routes.get(route)
''')

base_lambda_routes = Definition("{{lambda_name}}/routes.py", text='''from lambda_routing import lambda_route, authenticated_lambda_route

@lambda_route('/{{lambda_name}}/hello')
def hello(data):
	if data.get('msg') is None:
		print('[-] missing message')
		return { 'success': True, 'msg': 'hello world!' }
	else:
		return { 'success': True, 'msg': data['msg'] }
''')

base_lambda_build = Definition("{{lambda_name}}/build.sh", text='''#!/bin/bash
python3 -m venv my_venv
source my_venv/bin/activate
pip install --upgrade pyjwt --ignore-installed
deactivate

rm -rf modules __pycache__
mkdir modules
ls -la my_venv/lib/*/site-packages/
rm -rf my_venv/lib/*/site-packages/*.dist-info \
	my_venv/lib/*/site-packages/setuptools \
	my_venv/lib/*/site-packages/pip \
	my_venv/lib/*/site-packages/botocore \
	my_venv/lib/*/site-packages/*/__pycache__
cp -rf my_venv/lib/*/site-packages/* modules
rm -rf my_venv/

zip -r ../build/{{lambda_name}}.zip . -x build.sh -x "*__pycache__*"
''')

sqs_lambda_main = base_lambda_main.but_replace('''res = route_request(event)
		print('[r] response:', res)
		if res.get('success') is not None and res['success']:
			send_metric('lambda-success')
		else:
			send_metric('lambda-error')
		return res''', '''results = route_sqs_request(event)
		print('[r] response:', results)
		# track metrics
		for res in results:
			if res.get('success') is not None and res['success']:
				send_metric('lambda-success')
			else:
				send_metric('lambda-error')

		return results''')
sqs_lambda_module = base_lambda_module.and_append('''
output "{{lambda_name}}_sqs_queue_url" { value = module.{{lambda_name}}.sqs_queue_url }
''')
sqs_lambda_definition = base_lambda_definition.but_replace('''{
      Effect = "Allow"
      Action = "cloudwatch:PutMetricData"
      Resource = "*"
      Condition = {
        StringEquals = {
          "cloudwatch:namespace" = local.metrics_group
        }
      }
    }]''','''{
      Effect = "Allow"
      Action = "cloudwatch:PutMetricData"
      Resource = "*"
      Condition = {
        StringEquals = {
          "cloudwatch:namespace" = local.metrics_group
        }
      }
    }, {
      Effect = "Allow",
      Action = [
          "sqs:ReceiveMessage",
          "sqs:DeleteMessage",
          "sqs:GetQueueAttributes"
      ],
      Resource = aws_sqs_queue.input_queue.arn
    }]''').and_append('''
resource "aws_sqs_queue" "input_queue" {
  name        = "${local.fullname}-input_queue"
  delay_seconds             = 90
  max_message_size          = 2048
  message_retention_seconds = 86400
  receive_wait_time_seconds = 10
  redrive_policy = jsonencode({
    deadLetterTargetArn = aws_sqs_queue.input_dlq.arn
    maxReceiveCount     = 4
  })
  visibility_timeout_seconds = 300
}

resource "aws_sqs_queue" "input_dlq" {
  name        = "${local.fullname}-input_dlq"
}

resource "aws_lambda_event_source_mapping" "event_source_mapping" {
  event_source_arn = aws_sqs_queue.input_queue.arn
  function_name    = aws_lambda_function.lambda_function.arn
  batch_size = 1
}


output "sqs_queue_arn" { value = aws_sqs_queue.input_queue.arn }
output "sqs_queue_url" { value = aws_sqs_queue.input_queue.url }
''')
sqs_lambda_test = Definition("scripts/test.sh", append=True, text='''
SQS_URL=$(cat infrastructure/terraform.tfstate | jq -r '.outputs.{{lambda_name}}_sqs_queue_url.value')
aws sqs send-message --region us-east-1 --queue-url "$SQS_URL" --message-body "{\"action\":\"/{{lambda_name}}/hello\"}"
''')

api_lambda_module = base_lambda_module.and_append('''
output "{{lambda_name}}_api_url" { value = module.{{lambda_name}}.api_url }
''')
api_lambda_definition = base_lambda_definition.and_append('''
resource "aws_apigatewayv2_api" "gateway_api" {
  name          = "${local.fullname}-gateway_api"
  protocol_type = "HTTP"

  cors_configuration {
    allow_origins = ["*"]
    allow_methods = ["POST", "OPTIONS"]
    allow_headers = ["content-type", "authorization"]
    max_age = 300
  }

  # change this to true when you create a cloudfront distribution for the api
  disable_execute_api_endpoint = false
}

resource "aws_apigatewayv2_stage" "gateway_stage" {
  api_id = aws_apigatewayv2_api.gateway_api.id

  name        = "$default"
  auto_deploy = true

  access_log_settings {
    destination_arn = aws_cloudwatch_log_group.apigw_log_group.arn

    format = jsonencode({
      requestId               = "$context.requestId"
      sourceIp                = "$context.identity.sourceIp"
      requestTime             = "$context.requestTime"
      protocol                = "$context.protocol"
      httpMethod              = "$context.httpMethod"
      resourcePath            = "$context.resourcePath"
      routeKey                = "$context.routeKey"
      status                  = "$context.status"
      responseLength          = "$context.responseLength"
      integrationErrorMessage = "$context.integrationErrorMessage"
    })
  }
}

resource "aws_apigatewayv2_integration" "apigw_integration" {
  api_id = aws_apigatewayv2_api.gateway_api.id

  integration_uri    = aws_lambda_function.lambda_function.invoke_arn
  integration_type   = "AWS_PROXY"
  integration_method = "POST"
  payload_format_version = "2.0"
}

resource "aws_apigatewayv2_route" "apigw_route" {
  api_id = aws_apigatewayv2_api.gateway_api.id

  route_key = "POST /"
  target    = "integrations/${aws_apigatewayv2_integration.apigw_integration.id}"
}

resource "aws_cloudwatch_log_group" "apigw_log_group" {
  name = "/aws/api_gw/${aws_apigatewayv2_api.gateway_api.name}"
  retention_in_days = 90
}

resource "aws_lambda_permission" "apigw_permission" {
  statement_id  = "AllowExecutionFromAPIGateway"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.lambda_function.function_name
  principal     = "apigateway.amazonaws.com"

  source_arn = "${aws_apigatewayv2_api.gateway_api.execution_arn}/*/*"
}

# # domain mapping to enable a domain name
# variable "domain_acm_cert_arn" { type = string }
# resource "aws_apigatewayv2_domain_name" "apigw_domain" {
#   domain_name = "api.${local.domain_name}"
#   domain_name_configuration {
#     certificate_arn = var.domain_acm_cert_arn
#     endpoint_type   = "REGIONAL"
#     security_policy = "TLS_1_2"
#   }
# }

# resource "aws_apigatewayv2_api_mapping" "apigw_domain_mapping" {
#   api_id      = aws_apigatewayv2_api.gateway_api.id
#   domain_name = aws_apigatewayv2_domain_name.apigw_domain.id
#   stage  = aws_apigatewayv2_stage.gateway_stage.id
# }

# output "api_url" { value = "https://${aws_apigatewayv2_domain_name.apigw_domain.id}/" }
output "api_url" { value = "${aws_apigatewayv2_stage.gateway_stage.invoke_url}" }
''')
api_lambda_test = Definition("scripts/test.sh", append=True, text='''
INVOKE_URL=$(cat infrastructure/terraform.tfstate | jq -r '.outputs.{{lambda_name}}_api_url.value')
curl -X POST "$INVOKE_URL" -H 'content-type: application/json' -d '{"action":"/{{lambda_name}}/hello","msg":"working as intended"}'
''')


website_s3_module = Definition("infrastructure/main.tf", append=True, text='''
variable "{{website_name}}_build_path" { default = "../{{website_name}}/dist" }

module "{{website_name}}" {
  source = "./{{website_name}}"

  domain_name = "{{domain_name}}"
  build_directory = var.{{website_name}}_build_path
}

output "{{website_name}}_endpoint" { value = "${module.{{website_name}}.website_endpoint}" }
''')

website_s3_definition = Definition("infrastructure/{{website_name}}/{{website_name}}.tf", text='''
variable "domain_name" { type = string }
variable "build_directory" { type = string }

locals {
  content_types = {
    css  = "text/css"
    html = "text/html"
    js   = "application/javascript"
    json = "application/json"
    txt  = "text/plain"
  }
}

resource "aws_s3_bucket" "bucket" {
  bucket = var.domain_name
  force_destroy = true
}

resource "aws_s3_bucket_public_access_block" "bucket_access_block" {
  bucket = aws_s3_bucket.bucket.id

  block_public_acls       = false
  block_public_policy     = false
  ignore_public_acls      = false
  restrict_public_buckets = false
}

resource "aws_s3_bucket_policy" "bucket_policy" {
  bucket = aws_s3_bucket.bucket.id
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Principal = "*"
      Action = "s3:GetObject"
      Resource = "arn:aws:s3:::${var.domain_name}/*"
    }]
  })
}

resource "aws_s3_bucket_website_configuration" "bucket_website_configuration" {
  bucket = aws_s3_bucket.bucket.id

  index_document {
    suffix = "index.html"
  }
  error_document {
    key = "index.html"
  }
}

resource "aws_s3_object" "build_file" {
  for_each = fileset(var.build_directory, "**")

  bucket = aws_s3_bucket.bucket.id
  key    = each.value
  source = "${var.build_directory}/${each.value}"

  content_type = lookup(local.content_types, element(split(".", each.value), length(split(".", each.value)) - 1), "text/plain")
  etag   = filemd5("${var.build_directory}/${each.value}")
}

output "website_endpoint" { value = "${aws_s3_bucket_website_configuration.bucket_website_configuration.website_endpoint}" }



# # cf distribution fronting the website, enable when useful
# variable "domain_acm_cert_arn" { type = string }
# resource "aws_cloudfront_distribution" "s3_distribution" {
#   origin {
#     domain_name              = aws_s3_bucket_website_configuration.bucket_website_configuration.website_endpoint
#     # domain_name              = aws_s3_bucket.bucket.bucket_regional_domain_name
#     # origin_access_control_id = aws_cloudfront_origin_access_control.default.id
#     origin_id                = "website_origin"
#     custom_origin_config {
#       http_port                = 80
#       https_port               = 443
#       origin_keepalive_timeout = 5
#       origin_protocol_policy   = "http-only"
#       origin_read_timeout      = 30
#       origin_ssl_protocols     = [
#         "TLSv1.2",
#       ]
#     }
#   }

#   aliases = [ var.domain_name ]

#   enabled             = true
#   is_ipv6_enabled     = true
#   default_root_object = "index.html"

#   default_cache_behavior {
#     allowed_methods  = ["DELETE", "GET", "HEAD", "OPTIONS", "PATCH", "POST", "PUT"]
#     cached_methods   = ["GET", "HEAD"]
#     target_origin_id = "website_origin"

#     forwarded_values {
#       query_string = false

#       cookies {
#         forward = "none"
#       }
#     }

#     viewer_protocol_policy = "allow-all"
#     min_ttl                = 0
#     default_ttl            = 3600
#     max_ttl                = 86400
#   }

#   restrictions {
#     geo_restriction {
#       restriction_type = "none"
#       locations        = []
#     }
#   }

#   price_class = "PriceClass_100"

#   viewer_certificate {
#     acm_certificate_arn = var.domain_acm_cert_arn
#     ssl_support_method = "sni-only"
#   }
# }

# output "distribution_id" { value = "${aws_cloudfront_distribution.s3_distribution.id}" }
''')
website_s3_dockerfile = Definition("{{website_name}}/docker/Dockerfile", text='''#!/usr/bin/env -S bash -c "docker run -v \${PWD}:/app -p3000:3000 --rm -it \$(docker build -q docker) \$@"
FROM ubuntu:20.04
WORKDIR /app

ARG DEBIAN_FRONTEND=noninteractive
RUN apt-get update \
    && apt-get install -y curl nano jq zip

RUN curl -sL https://deb.nodesource.com/setup_14.x | bash -

# install postgresql
ARG DEBIAN_FRONTEND=noninteractive
RUN apt-get update \
    && apt-get install -y awscli \
    && apt-get install -y openssh-client \
    && apt-get install -y nodejs \
    && apt autoclean \
    && apt autoremove \
    && apt clean

RUN useradd -ms /bin/bash runuser
USER runuser

CMD npm start
''')
website_s3_package_json = Definition("{{website_name}}/package.json", text='''{
  "name": "react-site",
  "version": "1.0.0",
  "description": "",
  "main": "index.js",
  "scripts": {
    "start": "webpack serve",
    "build": "rm -rf dist && webpack . --mode production && cp -rf public/* dist && rm dist/**.map",
    "test": "test"
  },
  "author": "",
  "devDependencies": {
    "@babel/cli": "^7.19.3",
    "@babel/core": "^7.20.5",
    "@babel/eslint-parser": "^7.19.1",
    "@babel/plugin-transform-runtime": "^7.19.6",
    "@babel/preset-env": "^7.20.2",
    "@babel/preset-react": "^7.18.6",
    "@babel/runtime": "^7.20.6",
    "babel-loader": "^9.1.0",
    "html-webpack-plugin": "^5.5.0",
    "webpack": "^5.75.0",
    "webpack-cli": "^5.0.0",
    "webpack-dev-server": "^4.11.1"
  },
  "dependencies": {
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "react-router-dom": "^6.4.4"
  }
}''')
website_s3_webpack_config = Definition("{{website_name}}/webpack.config.js", text='''const path = require("path");
const HtmlWebpackPlugin = require("html-webpack-plugin");

/*We are basically telling webpack to take index.js from entry. Then check for all file extensions in resolve. 
After that apply all the rules in module.rules and produce the output and place it in main.js in the public folder.*/

module.exports={
    /** "mode"
     * the environment - development, production, none. tells webpack 
     * to use its built-in optimizations accordingly. default is production 
     */
    mode: "development", 
    /** "entry"
     * the entry point 
     */
    entry: "./index.js", 
    devtool: 'source-map',
    output: {
        filename: "bundle.[hash].js",
        path: path.resolve(__dirname, "dist"),
    },
    /** "target"
     * setting "node" as target app (server side), and setting it as "web" is 
     * for browser (client side). Default is "web"
     */
    target: "web",
    devServer: {
        /** "port" 
         * port of dev server
        */
        port: "3000",
        /** "static" 
         * This property tells Webpack what static file it should serve
        */
        static: ["./public"],
        /** "open" 
         * opens the browser after server is successfully started
        */
        open: true,
        /** "hot"
         * enabling and disabling HMR. takes "true", "false" and "only". 
         * "only" is used if enable Hot Module Replacement without page 
         * refresh as a fallback in case of build failures
         */
        hot: false ,
        /** "liveReload"
         * disable live reload on the browser. "hot" must be set to false for this to work
        */
        liveReload: true,
        historyApiFallback: true,
    },
    plugins: [
        new HtmlWebpackPlugin({
          template: "./src/index.html",
        }),
    ],
    resolve: {
        /** "extensions" 
         * If multiple files share the same name but have different extensions, webpack will 
         * resolve the one with the extension listed first in the array and skip the rest. 
         * This is what enables users to leave off the extension when importing
         */
        modules: [__dirname, "src", "node_modules"],
        extensions: ['.js','.jsx','.json', ".tsx", ".ts"] 
    },
    module:{
        /** "rules"
         * This says - "Hey webpack compiler, when you come across a path that resolves to a '.js or .jsx' 
         * file inside of a require()/import statement, use the babel-loader to transform it before you 
         * add it to the bundle. And in this process, kindly make sure to exclude node_modules folder from 
         * being searched"
         */
        rules: [
            {
                test: /\.(js|jsx)$/,    //kind of file extension this rule should look for and apply in test
                exclude: /node_modules/, //folder to be excluded
                use:  'babel-loader' //loader which we are going to use
            }
        ]
    }
}''')
website_s3_babelrc = Definition("{{website_name}}/.babelrc", text='''{
    /*
        a preset is a set of plugins used to support particular language features.
        The two presets Babel uses by default: es2015, react
    */
    "presets": [
        "@babel/preset-env", //compiling ES2015+ syntax
        "@babel/preset-react" //for react
    ],
    /*
        Babel's code transformations are enabled by applying plugins (or presets) to your configuration file.
    */
    "plugins": [
        "@babel/plugin-transform-runtime"
    ]
}''')
website_s3_index_js = Definition("{{website_name}}/index.js", text='''import React from "react";
import reactDom from "react-dom";
import { createRoot } from 'react-dom/client';
import { BrowserRouter } from "react-router-dom";
import App from "./src/App"

const root = createRoot(document.getElementById('root'));
root.render(<BrowserRouter>
		<App />
	</BrowserRouter>);
''')
website_s3_index_html = Definition("{{website_name}}/src/index.html", text='''<html lang="en">
	<head>
		<meta charset="UTF-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<title>{{website_name}}</title>
		<link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">
		<link rel="stylesheet" type="text/css" href="css/styles.css">
	</head>
	<body>
		<div id="root"></div>
	</body>
</html>''')
website_s3_app_js = Definition("{{website_name}}/src/App.js", text='''import React from "react";
import { Routes, Route, Navigate } from "react-router-dom";
import Home from "./pages/Home";

const App = () => {
	return (
		<div className="app">
			<Routes>
				<Route path="/" element={ <Home/> } />
			</Routes>
		</div>
	)
}

export default App
''')
website_s3_home_js = Definition("{{website_name}}/src/pages/Home.jsx", text='''import React from "react";

const Home = () => {
	return (
		<div className="homepage">
			hello world
		</div>
	)
}
export default Home
''')
website_s3_styles_css = Definition("{{website_name}}/public/css/styles.css", text='''''')
website_s3_test = Definition("scripts/test.sh", append=True, text='''
WEBSITE_URL=$(cat infrastructure/terraform.tfstate | jq -r '.outputs.{{website_name}}_endpoint.value')
echo "website endpoint: $WEBSITE_URL"
curl "$WEBSITE_URL"
''')



ec2_server_module = Definition("infrastructure/main.tf", append=True, text='''
variable "{{server_name}}_build_path" { default = "../build/{{server_name}}.zip" }
variable "{{server_name}}_ssh_keypath" { default = "../.keys/{{server_name}}_key" }

module "{{server_name}}" {
  source = "./{{server_name}}"

  metrics_path = local.metrics_path
  infragroup_fullname = local.infragroup_fullname
  package_build_path = var.{{server_name}}_build_path
  ssh_keypath = var.{{server_name}}_ssh_keypath
  server_config = {}
}

output "{{server_name}}_server_ip" { value = module.{{server_name}}.server_ip }
output "{{server_name}}_instance_id" { value = module.{{server_name}}.instance_id }
''')
ec2_server_makefile = Definition("./Makefile", append=True, text='''
build_{{server_name}}_package:
	-mkdir build
	zip build/{{server_name}}.zip -FSr {{server_name}}/
	./scripts/create-keys.sh "{{server_name}}_key"
''')
ec2_server_definition = Definition("infrastructure/{{server_name}}/{{server_name}}.tf", text='''
variable "infragroup_fullname" { type = string }
variable "metrics_path" { type = string }

variable "package_build_path" { type = string }
variable "ssh_keypath" { type = string }
variable "ingress_ports" { default = [22] }
variable "server_config" { default = {} }

locals {
  fullname = "${var.infragroup_fullname}-{{server_name}}"
  metrics_group = "${var.metrics_path}/{{server_name}}"
}

resource "aws_instance" "instance" {
  ami           = "ami-0e14491966b97e8bc"
  instance_type = "t2.medium"

  security_groups = [aws_security_group.instance_security_group.name]
  key_name = aws_key_pair.instance_aws_key.id
  iam_instance_profile = aws_iam_instance_profile.instance_profile.id

  root_block_device {
    volume_size = 20
  }

  user_data = <<-EOT
    #cloud-config
    runcmd:
      - sleep 5
      - sudo env DEBIAN_FRONTEND=noninteractive apt -y update
      - sudo env DEBIAN_FRONTEND=noninteractive apt install -y unzip curl awscli
      - sudo mkdir -p /app/log
      - sudo mkdir -p /app/config
      - sudo chown -R ubuntu:ubuntu /app
      - echo '${jsonencode(var.server_config)}' > /app/config/server_config.json
      - echo 's3://${aws_s3_bucket.deploy_bucket.id}/'
      - aws s3 cp s3://${aws_s3_bucket.deploy_bucket.id}/package.zip /app/package.zip
      - unzip /app/package.zip -d /app
      - sudo chown -R ubuntu:ubuntu /app
      - cd /app/{{server_name}} && chmod +x *.sh && ./install.sh 2>&1 >> /app/log/init.log
    EOT
}

resource "aws_key_pair" "instance_aws_key" {
  key_name_prefix = "${local.fullname}-aws_key-"
  public_key = file("${var.ssh_keypath}.pub")
}

resource "aws_security_group" "instance_security_group" {
  name        = "Allow web traffic"
  description = "Allow ssh and standard http/https ports inbound and everything outbound"

  dynamic "ingress" {
    for_each = var.ingress_ports
    iterator = port
    content {
      from_port   = port.value
      to_port     = port.value
      protocol    = "TCP"
      cidr_blocks = ["0.0.0.0/0"]
    }
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_iam_instance_profile" "instance_profile" {
  name = "${local.fullname}-instance_profile"
  role = aws_iam_role.instance_role.name
}

resource "aws_iam_role" "instance_role" {
  name = "${local.fullname}-instance_role"
  path = "/"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "ec2.amazonaws.com"
      }
    }]
  })
}

resource "aws_iam_policy" "instance_policy" {
  name = "${local.fullname}-instance_policy"

  policy = <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": [
        "s3:Get*",
        "s3:List*"
      ],
      "Effect": "Allow",
      "Resource": [
        "arn:aws:s3:::${aws_s3_bucket.deploy_bucket.id}",
        "arn:aws:s3:::${aws_s3_bucket.deploy_bucket.id}/*"
      ]
    }
  ]
}
EOF
}

resource "aws_iam_policy_attachment" "instance_policy_attachment" {
  name       = "${local.fullname}-instance_policy_attachment"
  roles      = [aws_iam_role.instance_role.name]
  policy_arn = aws_iam_policy.instance_policy.arn
}


resource "random_id" "deploy_bucket_random_id" { byte_length = 24 }
resource "aws_s3_bucket" "deploy_bucket" {
  bucket = "deploypackage-${random_id.deploy_bucket_random_id.hex}"
  force_destroy = true
}

resource "aws_s3_object" "build_file" {
  bucket = aws_s3_bucket.deploy_bucket.id
  key = "package.zip"
  source = var.package_build_path
  etag = filemd5(var.package_build_path)
}


output "server_ip" { value = "${aws_instance.instance.public_ip}" }
output "instance_id" { value = "${aws_instance.instance.id}" }
''')
ec2_server_install_sh = Definition("{{server_name}}/install.sh", text='''#!/bin/sh
echo "[+] chmoding everything"
cd /app/package
chmod +x ./*.sh
echo "[+] installing reboot script"
sudo ln -s /app/package/reboot.sh /var/lib/cloud/scripts/per-boot/reboot-init.sh
echo "[+] running main server"
./server-startup.sh
''')


infra_base_template = TemplateDefinitions('{infra_name} infra', {
	'company_name': r"^[a-zA-Z_]+$",
	'infra_name': r"^[a-zA-Z_][a-zA-Z0-9_\-]+$"
}, [
	base_terraform_main,
	base_makefile,
], '''
base infrastructure established
deploy with `weasel deploy`
destroy and clean with `weasel destroy clean`
''')


ec2_server_template = TemplateDefinitions('{server_name} server', {
	'server_name': r"^[a-zA-Z_][a-zA-Z0-9_]+$",
}, [
	ec2_server_module,
	ec2_server_makefile,
	ec2_server_definition,
	ec2_server_install_sh,
], '''
ec2 server scaffolding established...
build with `weasel build_{server_name}_package`
your private ssh key will be located at `.keys/{server_name}_key` after the build
ssh into the server with `./scripts/ssh-into.sh {server_name}` after it is deployed
''')


website_s3_bucket_template = TemplateDefinitions('{website_name} bucket', {
	'website_name': r"^[a-zA-Z_][a-zA-Z0-9_]+$",
	'domain_name': r"^[a-zA-Z_][a-zA-Z0-9_]+(\.[a-zA-Z0-9_]+)+$",
}, [
	website_s3_module,
	website_s3_definition,
	website_s3_dockerfile,
	website_s3_package_json,
	website_s3_webpack_config,
	website_s3_babelrc,
	website_s3_index_js,
	website_s3_index_html,
	website_s3_app_js,
	website_s3_home_js,
	website_s3_styles_css,
	website_s3_test,
], '''
website bucket scaffolding established...
build the website by doing:
	cd {website_name}
	chmod +x docker/Dockerfile
	./docker/Dockerfile npm i -dev
	./docker/Dockerfile npm run build
then deploy your infra as normal
''')


base_lambda_template = TemplateDefinitions('{lambda_name} lambda', { 'lambda_name': r"^[a-zA-Z_][a-zA-Z0-9_]+$" }, [
	base_lambda_definition,
	base_lambda_makefile,
	base_lambda_module,
	base_lambda_main,
	base_lambda_lib,
	base_lambda_routes,
	base_lambda_build,
], '''
basic lambda scaffolded...
build the lambda package with `weasel build_{lambda_name}`
''')


api_lambda_template = TemplateDefinitions('{lambda_name} lambda', { 'lambda_name': r"^[a-zA-Z_][a-zA-Z0-9_]+$" }, [
	api_lambda_definition,
	base_lambda_makefile,
	api_lambda_module,
	base_lambda_main,
	base_lambda_lib,
	base_lambda_routes,
	base_lambda_build,
	api_lambda_test,
], '''
sqs lambda scaffolded...
build the lambda package with `weasel build_{lambda_name}`
test the lambda by executing `./scripts/test.sh`
''')


sqs_lambda_template = TemplateDefinitions('{lambda_name} lambda', { 'lambda_name': r"^[a-zA-Z_][a-zA-Z0-9_]+$" }, [
	sqs_lambda_definition,
	base_lambda_makefile,
	sqs_lambda_module,
	sqs_lambda_main,
	base_lambda_lib,
	base_lambda_routes,
	base_lambda_build,
	sqs_lambda_test,
], '''
sqs lambda scaffolded...
build the lambda package with `weasel build_{lambda_name}`
test the lambda by executing `./scripts/test.sh`
''')



# wrap to catch sigint
try:
	# parse arguments
	parser = argparse.ArgumentParser(prog='stonemill', description='A terraform scaffolding tool')
	parser.add_argument('--infra-base', nargs=2, help='creates the starter Makefile and main.tf;\t stonemill --infra-base mycompany myproject')
	parser.add_argument('--lambda-function', nargs=1, help='creates a py3 lambda;\t stonemill --lambda-function my_lambda')
	parser.add_argument('--api-lambda-function', nargs=1, help='creates a py3 lambda with an api gateway;\t stonemill --api-lambda-function my_lambda')
	parser.add_argument('--sqs-lambda-function', nargs=1, help='creates a py3 lambda with an sqs input queue;\t stonemill --sqs-lambda-function my_lambda')
	parser.add_argument('--website-s3-bucket', nargs=2, help='creates an s3 bucket for hosting a react site;\t stonemill --website-s3-bucket my_website myspecialfrontend.com')
	parser.add_argument('--ec2-server', nargs=1, help='creates an ec2 server with ssh key;\t stonemill --ec2-server my_server')
	args = parser.parse_args()

	if args.infra_base:
		template_args = infra_base_template.parse_arguments(company_name=args.infra_base[0], infra_name=args.infra_base[1])
		infra_base_template.mill_template(template_args)

	elif args.ec2_server:
		template_args = ec2_server_template.parse_arguments(server_name = args.ec2_server[0])
		ec2_server_template.mill_template(template_args)

	elif args.website_s3_bucket:
		template_args = website_s3_bucket_template.parse_arguments(website_name = args.website_s3_bucket[0], domain_name = args.website_s3_bucket[1])
		website_s3_bucket_template.mill_template(template_args)

	elif args.lambda_function:
		template_args = base_lambda_template.parse_arguments(lambda_name = args.lambda_function[0])
		base_lambda_template.mill_template(template_args)

	elif args.api_lambda_function:
		template_args = api_lambda_template.parse_arguments(lambda_name = args.api_lambda_function[0])
		api_lambda_template.mill_template(template_args)

	elif args.sqs_lambda_function:
		template_args = sqs_lambda_template.parse_arguments(lambda_name = args.sqs_lambda_function[0])
		sqs_lambda_template.mill_template(template_args)

	else:
		parser.print_help()

except KeyboardInterrupt:
	sys.exit(1)



